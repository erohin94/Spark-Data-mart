## Шаги проекта

Установить необходимые инструменты - [Настройка технической составляющей](https://github.com/erohin94/Spark-Data-mart/blob/main/SETUP_V2.md)

Открыть Jupyter, DBeaver - подключитсяк Postgre и ClickHouse

## Суть проекта

Представим что мы какой то бизнес, у нас есть свой сайт.

Что мы делаем? Мы создаем дизайн проект кухонь, делаем кухни на заказ.

Что мы хотим? Мы хотим больше продавать, юольше клиентов и соответственно больше заработать на масштабирование и развитие бизнеса.

Для этого нам нужна аналитика, чтобы мы могли понимать наше состояние, знать к чему движемся, чего можем достичь.

-Найти идеи, которые позволят получить доходность выше рынка.

-Отследить, какие каналы принесли продажи и их объем.

-Как эффективнее распределить бюджет и ресурсы.

На текущий момент в компании имеем следующее: много разныхи сточников данных, данные лежат в несовместимых форматах, большой объем данных, требуется ручная работа.

Чего хотим сделать: чтобы данные были объединены, агрегированы и собраны в одном месте для оперативной аналитики, построения дашбордов и поиска инсайтов.

## Описание исходных данных

```
Табличка           Место хранения     Комментарий
visits             ClickHouse         Посещение пользователяминашего сайта
costs              Postgres           Расходы на рекламу
campaigns_dict     csv                Словарик с сопоставлением рекламных кампаний
submits            parquet            Заполненные формы на сайте для заявки/обратного звонка
deals              parquet            Заказанные дизайн-проекты
```

Так много табличек, чтобы съэмулировать некую среду, где много источников и мы их будем обьединять в одном месте.

Методология построения витрины подробно описана в [ссылка](https://github.com/erohin94/Spark-Data-mart/blob/main/Methodology.md)

## Что буду использовать

`Postgre` транзакционную БД как источник данных 

`ClickHouse` аналитическая колоночная БД как источник данных

`Spark` движок для обработки больших данных

`JupyterNotebook` среда интерактивной разработки

`DBeaver` клиент для подключения к разным бд

`Metabase` инструмент для визуализации данных

## Сам проект

Открываем ноутбук и запускаем ячейки:

**1)Создаем спарк ссесию.**

Если не создать спарк ссесию, то не сможем работать с спарком.

Импортирую модуль `import pyspark.sql.functions as F` содержит основные функции преобразования, для всего что мы хотим, например чтобы посчитать какую то сумму и тд.

Импорт `from pyspark.sql import SparkSession` чтобы стартануть Spark.

Импорт `from pyspark.sql.window import Window` для написания оконных функций.

В `jar_files` передал пути с джарниками которые указаны в докер файле, это надо бля подключения к БД. 

Если открыть Spark UI. 

На вкладке Jobs будут SQL запросы которые я буду запускать.

Stages это подджобы, например некоторые преобразования требуют несколько Стейджей (2-х), но в основном одна джоба одно преобразование, в нашем случае.

Storage - хранится информация о том какие данные мы взяли в оперативную память.

Environment - можно посмотреть технические параметры. Можно увидеть например строку с джарниками которые я подгрузил.

<img width="1568" height="30" alt="image" src="https://github.com/user-attachments/assets/d6a4a019-fe7d-4a91-92d8-fb351a05b48e" />

А параметр `spark.master	local[*]` в Environment, звездочка указывает на то что для расчета будут использоваться все ядра моего ноутбука.

Executors - так как все запускаем не в кластерном режиме, а на ноутбуке то будет только один driver.

<img width="1901" height="558" alt="image" src="https://github.com/user-attachments/assets/c2fdca69-d5b3-4662-983d-a38153c0bfc2" />

Если бы работали в кластере, то был бы еще список из Executor - рабочие лошадки, которые преобразуют куски данных.

Вкладка SQL / DataFrame - будем смотреть в какие SQl запросы все преобразуется и как это графически отображается.


**2)Подключаемся к источникам**

Так как внутри контейнера Jupyter уже существует рабочая папка, которая по умолчанию примонтирована в `./notebooks:/home/jovyan/work`. 

То всё, что ты кладёшь в локальную папку notebooks (рядом с docker-compose.yml) — автоматически видно внутри Jupyter по пути `/home/jovyan/work`.

<img width="645" height="156" alt="image" src="https://github.com/user-attachments/assets/d0768cd4-6b09-43f2-abf0-5b526ee472fc" />

Поэтому все таблички и файлики буду собирать в папку на своем компьютере `C:\Users\erohi\Desktop\spark_data_mart\notebooks`, которая в юпитере видна по следующему пути `/home/jovyan/work/data/здесь_название_файла_например.csv`. 

<img width="633" height="111" alt="image" src="https://github.com/user-attachments/assets/9c7bf506-2e2a-4725-b4c3-219568aa613f" />

Суть ленивых вычислений - есть трансформации(transformations) и действия (actions).

При запуске ячейки с `campaigns_dict = (spark.read.option('header', True).csv(f'{path}/campaigns_dict.csv'))` у нас ничего не произойдет - это трансформация.

Но если запустить `campaigns_dict.show(5, truncate=False)` - это действие(метод show), то увидим табличку.

Метод `.printSchema()` - выводит описательные характеритики.

<img width="341" height="87" alt="image" src="https://github.com/user-attachments/assets/c95a56fa-1ef8-496d-873d-463533b0733c" />

Особенность parquet перед csv, это как минимум то что мы можем посмотреть у паркета метаданные. Для этого есть библиотека `pyarrow`

`num_row_groups: 1` - это техническая составляющая паркет файлов. Несколько строк обьединяются в строковые группы, на основе их размера. По стандарту это 512мб или 1гб. В данном случае данных мало и поэтому они поместились в одну группу row_groups. Если данных будет больше, то будет больше групп.

**3)postgres** 35:00

Проверить права на схему через системные таблицы, чтобы можно было созавать таблицы и загружать с помощью spark.

```
SELECT nspname AS schema_name,
       nspowner::regrole AS owner,
       nspacl
FROM pg_namespace
WHERE nspname = 'public'
```

Если будет следующее

<img width="702" height="27" alt="image" src="https://github.com/user-attachments/assets/1191f2b2-4a9f-49d3-9e6b-67b2f044e523" />

| Поле            | Значение                                                                                                                          |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| **schema_name** | `public` — стандартная схема                                                                                                      |
| **owner**       | `pg_database_owner` — это не пользователь `admin`, а *роль-владелец базы данных*                                             |
| **nspacl**      | `{pg_database_owner=UC/pg_database_owner,=U/pg_database_owner}` — только владельцу разрешено `U` (использование) и `C` (создание) |

`U = USAGE` (можно использовать схему)

`C = CREATE` (можно создавать таблицы)

Это значит пользователь `admin` не является владельцем схемы `public`
и не имеет прав `CREATE` на неё. Поэтому Spark не сможет создать таблицу при записи — и, скорее всего, выдаст ошибку вроде
`permission denied for schema public`.

Что делаю, выполняю команду в терминале `docker exec -it postgres psql -U admin -d mydb`

<img width="501" height="51" alt="image" src="https://github.com/user-attachments/assets/072a9c22-e9e8-4efd-b6c4-beacac9475d4" />

А затем внутри psql выполнить:

```
ALTER SCHEMA public OWNER TO admin;
GRANT CREATE ON SCHEMA public TO admin;
```

<img width="369" height="83" alt="image" src="https://github.com/user-attachments/assets/9466cab5-5a6e-4b00-8445-0b6773d05edb" />

Проверяю и вижу следующее

<img width="626" height="289" alt="image" src="https://github.com/user-attachments/assets/f78ad52f-0043-4bfc-9a4e-db0413655a50" />

То есть у пользователя admin есть и U, и C. А владелец схемы теперь тоже admin.

Далее, загружу таблицу в postgre. Для этого в локальную папку кладу файлик. (Это будет одна из таблиц источников, на основе которой будем собирать витрину)

<img width="627" height="153" alt="image" src="https://github.com/user-attachments/assets/c55a89c6-9bbe-4b10-9351-8fb61159caa4" />

Читаю этот файл в спарк и сохраняю в postgre. `.mode("overwrite")` создает автоматически таблицу на основе схемы датафрейма при чтении CSV.

Проверяю, как видно, таблица есть в postgre:

<img width="570" height="199" alt="image" src="https://github.com/user-attachments/assets/270c5c7c-8b83-4c6b-99b2-04d0664f8a37" />

**4) ClickHouse**

Для начала так же загружу таблицу `visits_clickhouse.csv` в ClickHouse (Это будет одна из таблиц источников, на основе которой будем собирать витрину). Так же добавляю CSV файл в локальную папку, где лежат все таблички. 

<img width="641" height="189" alt="image" src="https://github.com/user-attachments/assets/8c783cc0-37e0-45c1-b31a-d14a580aaba7" />

Создаю в ручную таблицу в клике, в дибивере прописываю следующее:

```
/* Создаю таблицу */
CREATE TABLE default.visits
(
    visitid Int32,
    visitDateTime DateTime,
    URL String,
    duration Int32,
    clientID Int32,
    source String,
    UTMCampaign String,
    params String
)
ENGINE = MergeTree()
ORDER BY visitid
```

`visitid` — колонка для `ORDER BY`, по которой ClickHouse будет сортировать данные.

Далее с помощью Spark загружаю CSV файл в БД клик. И проверяю что данные добавились.

<img width="1237" height="266" alt="image" src="https://github.com/user-attachments/assets/4b23792b-e569-414d-abc1-9f6f01d0d627" />

После чего могу с помощбю спарк, работать с таблицей в клике.

## Готовим источники

У нас есть [методология](https://github.com/erohin94/Spark-Data-mart/blob/main/Methodology.md#3-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%8F-%D1%80%D0%B0%D1%81%D1%87%D0%B5%D1%82%D0%B0) расчета. Загоняю скрипты диалекта SQL в GPT и преобразую их в диалект Spark.


## Собираем витрину

Концепция "One Big Table" — это подход в аналитике и обработке данных, когда вся нужная информация собирается в одной таблице, готовой для аналитических запросов, отчетов или построения моделей. 

Делаем просто огромные широкие таблицы с кучей столбцов, с этим удобнее работать, чем каждый раз делать кучу джоинов чтобы получить нужные данные.

`customer_detailed.cache()` - кэширует DataFrame в памяти в Spark.

Spark помечает DataFrame, что его нужно сохранить в кэше при первом действии (action).

Как работает по шагам:

1.`cache()` ничего не делает сразу. Оно лишь ставит пометку «сохранить результат».

2.Когда впервые вызываешь `action`: `count()`, `show()`, `collect()`, `write()`, `display()`. Spark вычисляет DataFrame и кладёт результат в память `executors`.

3.Далее любые повторные операции над этим DataFrame: выполняются быстрее, Spark не будет заново перестраивать `lineage DAG` и перечитывать источники/фильтры/джоины.

Если перейти в Spark UI то будут видны все наши SQL запросы которые выполнялись, на вкладке JOBS, то есть высокоуровневые вычислительные задачи, которые Spark создаёт при выполнении action (count, show, write, collect, display и т.д.).

Перейдем на вкладку Storage и увидим как раз кеширование.

<img width="1902" height="378" alt="image" src="https://github.com/user-attachments/assets/b3b6188e-0e43-4c55-bf5b-9fdd7a92a966" />

В левой части показывает какой запрос делали. `Size in Memory` количество занятой пямяти. `Fraction Cached` сколько процентов закешировали.

Поэтому если будем вызывать какие то дейсвия, то они будут моментально отрабатываться, потому что все закешировалось в памяти.

Аналогично кешируе другие таблицы. 

После всех преобразований, сохранем витрины в Postgre. Использую функцию `def save_to_postgres(df, table_name):`

После того как собрали витрину и загрузили в БД, работа DE закончена. Далее уже с этими таблицами работают аналитики.

Но в рамках данного примера. Посчитаю метрики.

## Считаем метрики и анализируем результаты

1. Кампании без выручки. Получили список компаний, сколько на них потратили total_costs и сколько выручки эти компании принесли unique_deals.
   И считаем агрегированное количество компаний и общую сумму, сколько затратили. Потратили 149956.85, всего 13 компаний и они ничего нам не принесли.

2. Средняя цена сделки, для компаний которые что то принесли. unique_deals количество сделок, которые принесла компания. avg_deal_cost - сколько нам стоила эта сделка.

3. Убыточные кампании. Пусть каждая сделка стоит 5к.

4. Самые прибыльные кампании.

5. Метрики в разбивке по месяцам.

6. Сколько всего потратили денег на рекламу за год.


В самом конце делаю

```
# Spark UI
customer_detailed.unpersist()
campaigns_agg.unpersist()
dates_agg.unpersist()
```
У нас было закешировано три таблицы

<img width="1916" height="777" alt="image" src="https://github.com/user-attachments/assets/3423ffa8-f15c-4579-a875-065898a8ce3c" />

После того как выполнил команды `unpersist()`, обновляю UI и вижу что все очистилось.

<img width="812" height="216" alt="image" src="https://github.com/user-attachments/assets/ce733afa-aaa9-4ed6-af4e-08847fd034c9" />

Spark: удаляет кэшированные данные из памяти executors, освобождает RAM (и диск, если использовался MEMORY_AND_DISK), сбрасывает lineage пометку «DataFrame закэширован». Важно: сам объект DataFrame не удаляется — просто он больше не хранится в кэше.

И так же завершаю спрак сессию, чтобы освободить все ресурсы, ядра и тд. `spark.stop()`. Освободил и отдал свои ресурсы на общее пользование.

**Чему научились**

•понимать роль дата-инженера в потоке поставки данных

•разбираться в ТЗ и методологиях

•работать со Spark,применять функции в Spark

•читать из файлов и баз данных

•писать в файлы и базы данных

•преобразовывать диалекты SQL и pandas на Spark DataFrameAPI

•использовать Spark UI

•собирать витрину данных

•интерпретировать полученные результаты

**Изученные функции в Spark**

*Из модуля pyspark.sql.functions:*

•col–обращение к столбцу

•lit–столбец в виде конкретного значения

•substring –выделениеподстроки

•concat–объединение столбцов

•split–разделение столбца

•date_format–преобразование формата даты

•regexp_replace–замена по регулярке

•md5–хеширование столбца алгоритмом md5

•sum–суммирование

•countDistinct–подсчет уникальных значений

•when/otherwise –case when

*Над датафреймом:*

•select –выбор столбцов

•where –фильтр

•groupBy–группировка по столбцам

•agg–агрегация

•join–объединение таблиц

•sort–сортировка

•withColumn–добавление нового столбца

•cache–кэширование датафреймав памяти

•unpersist–освобождение ресурсов

•show –просмотр датафрейма

•printSchema–структура датафрейма

•count –количество строк

•distinct–отбор уникальных значений

•withColumnRenamed–переименование столбца

•drop–удаление столбца

*Над столбцом:*
•alias –псевдоним

•cast –преобразование к типу данных

•isin–нахождение значения столбца в списке значений

•rlike–сопоставление по регулярке

•getItem–получение элемента из массива

•between–фильтр между двумя границами


## Metabase

Далее на основе этого можно построить дашборд в Metabase. Подключаем его к Postgre. Вводим все теже данные что и в докер файле. Пароль `1111111`

1:41:32
