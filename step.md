## Шаги проекта

Установить необходимые инструменты - [Настройка технической составляющей](https://github.com/erohin94/Spark-Data-mart/blob/main/SETUP_V2.md)

Открыть Jupyter, DBeaver - подключитсяк Postgre и ClickHouse

## Описание исходных данных

```
Табличка           Место хранения     Комментарий
visits             ClickHouse         Посещение пользователяминашего сайта
costs              Postgres           Расходы на рекламу
campaigns_dict     csv                Словарикс сопоставлением рекламных кампаний
submits            parquet            Заполненные формы на сайте для заявки/обратного звонка
deals              parquet            Заказанные дизайн-проекты
```

Так много табличек, чтобы съэмулировать некую среду, где много источников и мы их будем обьединять в одном месте.

Методология построения витрины подробно описана в [ссылка](https://github.com/erohin94/Spark-Data-mart/blob/main/Methodology.md)

## Что буду использовать

`Postgre` транзакционную БД как источник данных 

`ClickHouse` аналитическая колоночная БД как источник данных

`Spark` движок для обработки больших данных

## Сам проект

Открываем ноутбук и запускаем ячейки:

**1)Создаем спарк ссесию.**

Если не создать спарк ссесию, то не сможем работать с спарком.

Импортирую модуль `import pyspark.sql.functions as F` содержит основные функции преобразования, для всего что мы хотим, например чтобы посчитать какую то сумму и тд.

Импорт `from pyspark.sql import SparkSession` чтобы стартануть Spark.

Импорт `from pyspark.sql.window import Window` для написания оконных функций.

В `jar_files` передал пути с джарниками которые указаны в докер файле, это надо бля подключения к БД. 

Если открыть Spark UI. 

На вкладке Jobs будут SQL запросы которые я буду запускать.

Stages это подджобы, например некоторые преобразования требуют несколько Стейджей (2-х), но в основном одна джоба одно преобразование, в нашем случае.

Storage - хранится информация о том какие данные мы взяли в оперативную память.

Environment - можно посмотреть технические параметры. Можно увидеть например строку с джарниками которые я подгрузил.

<img width="1568" height="30" alt="image" src="https://github.com/user-attachments/assets/d6a4a019-fe7d-4a91-92d8-fb351a05b48e" />

А параметр `spark.master	local[*]` в Environment, звездочка указывает на то что для расчета будут использоваться все ядра моего ноутбука.

Executors - так как все запускаем не в кластерном режиме, а на ноутбуке то будет только один driver.

**2)Подключаемся к источникам**



<img width="1901" height="558" alt="image" src="https://github.com/user-attachments/assets/c2fdca69-d5b3-4662-983d-a38153c0bfc2" />

Если бы работали в кластере, то был бы еще список из Executor - рабочие лошадки, которые преобразуют куски данных.

Вкладка SQL / DataFrame - будем смотреть в какие SQl запросы все преобразуется и как это графически отображается.
