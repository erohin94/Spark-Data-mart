## Шаги проекта

Установить необходимые инструменты - [Настройка технической составляющей](https://github.com/erohin94/Spark-Data-mart/blob/main/SETUP_V2.md)

Открыть Jupyter, DBeaver - подключитсяк Postgre и ClickHouse

## Суть проекта

Представим что мы какой то бизнес, у нас есть свой сайт.

Что мы делаем? Мы создаем дизайн проект кухонь, делаем кухни на заказ.

Что мы хотим? Мы хотим больше продавать, юольше клиентов и соответственно больше заработать на масштабирование и развитие бизнеса.

Для этого нам нужна аналитика, чтобы мы могли понимать наше состояние, знать к чему движемся, чего можем достичь.

-Найти идеи, которые позволят получить доходность выше рынка.

-Отследить, какие каналы принесли продажи и их объем.

-Как эффективнее распределить бюджет и ресурсы.

На текущий момент в компании имеем следующее: много разныхи сточников данных, данные лежат в несовместимых форматах, большой объем данных, требуется ручная работа.

Чего хотим сделать: чтобы данные были объединены, агрегированы и собраны в одном месте для оперативной аналитики, построения дашбордов и поиска инсайтов.

## Описание исходных данных

```
Табличка           Место хранения     Комментарий
visits             ClickHouse         Посещение пользователяминашего сайта
costs              Postgres           Расходы на рекламу
campaigns_dict     csv                Словарикс сопоставлением рекламных кампаний
submits            parquet            Заполненные формы на сайте для заявки/обратного звонка
deals              parquet            Заказанные дизайн-проекты
```

Так много табличек, чтобы съэмулировать некую среду, где много источников и мы их будем обьединять в одном месте.

Методология построения витрины подробно описана в [ссылка](https://github.com/erohin94/Spark-Data-mart/blob/main/Methodology.md)

## Что буду использовать

`Postgre` транзакционную БД как источник данных 

`ClickHouse` аналитическая колоночная БД как источник данных

`Spark` движок для обработки больших данных

`JupyterNotebook` среда интерактивной разработки

`DBeaver` клиент для подключения к разным бд

`Metabase` инструмент для визуализации данных

## Сам проект

Открываем ноутбук и запускаем ячейки:

**1)Создаем спарк ссесию.**

Если не создать спарк ссесию, то не сможем работать с спарком.

Импортирую модуль `import pyspark.sql.functions as F` содержит основные функции преобразования, для всего что мы хотим, например чтобы посчитать какую то сумму и тд.

Импорт `from pyspark.sql import SparkSession` чтобы стартануть Spark.

Импорт `from pyspark.sql.window import Window` для написания оконных функций.

В `jar_files` передал пути с джарниками которые указаны в докер файле, это надо бля подключения к БД. 

Если открыть Spark UI. 

На вкладке Jobs будут SQL запросы которые я буду запускать.

Stages это подджобы, например некоторые преобразования требуют несколько Стейджей (2-х), но в основном одна джоба одно преобразование, в нашем случае.

Storage - хранится информация о том какие данные мы взяли в оперативную память.

Environment - можно посмотреть технические параметры. Можно увидеть например строку с джарниками которые я подгрузил.

<img width="1568" height="30" alt="image" src="https://github.com/user-attachments/assets/d6a4a019-fe7d-4a91-92d8-fb351a05b48e" />

А параметр `spark.master	local[*]` в Environment, звездочка указывает на то что для расчета будут использоваться все ядра моего ноутбука.

Executors - так как все запускаем не в кластерном режиме, а на ноутбуке то будет только один driver.

<img width="1901" height="558" alt="image" src="https://github.com/user-attachments/assets/c2fdca69-d5b3-4662-983d-a38153c0bfc2" />

Если бы работали в кластере, то был бы еще список из Executor - рабочие лошадки, которые преобразуют куски данных.

Вкладка SQL / DataFrame - будем смотреть в какие SQl запросы все преобразуется и как это графически отображается.


**2)Подключаемся к источникам**

Так как внутри контейнера Jupyter уже существует рабочая папка, которая по умолчанию примонтирована в `./notebooks:/home/jovyan/work`. 

То всё, что ты кладёшь в локальную папку notebooks (рядом с docker-compose.yml) — автоматически видно внутри Jupyter по пути `/home/jovyan/work`.

<img width="645" height="156" alt="image" src="https://github.com/user-attachments/assets/d0768cd4-6b09-43f2-abf0-5b526ee472fc" />

Поэтому все таблички и файлики буду собирать в папку на своем компьютере `C:\Users\erohi\Desktop\spark_data_mart\notebooks`, которая в юпитере видна по следующему пути `/home/jovyan/work/data/здесь_название_файла_например.csv`. 

<img width="633" height="111" alt="image" src="https://github.com/user-attachments/assets/9c7bf506-2e2a-4725-b4c3-219568aa613f" />

Суть ленивых вычислений - есть трансформации(transformations) и действия (actions).

При запуске ячейки с `campaigns_dict = (spark.read.option('header', True).csv(f'{path}/campaigns_dict.csv'))` у нас ничего не произойдет - это трансформация.

Но если запустить `campaigns_dict.show(5, truncate=False)` - это действие(метод show), то увидим табличку.

Метод `.printSchema()` - выводит описательные характеритики.

<img width="341" height="87" alt="image" src="https://github.com/user-attachments/assets/c95a56fa-1ef8-496d-873d-463533b0733c" />

